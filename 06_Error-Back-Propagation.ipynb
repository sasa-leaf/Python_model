{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9173b4f",
   "metadata": {},
   "source": [
    "# 逆伝播のイメージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d188c",
   "metadata": {},
   "source": [
    "以下は、計算グラフによる逆伝播のイメージ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bbde0",
   "metadata": {},
   "source": [
    "![](image/06_calculation-graph.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4256b47",
   "metadata": {},
   "source": [
    "## 連鎖律"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ed01e",
   "metadata": {},
   "source": [
    "$$\\dfrac{\\partial L}{\\partial x}=\\dfrac{\\partial L}{\\partial t} \\dfrac{\\partial t}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d35a24",
   "metadata": {},
   "source": [
    "$x$ に関する$L$の微分は、「 $t$ に関する$L$の微分」と「 $x$ に関する $t$ の微分」の積で表すことができる。<br>\n",
    "逆方向に、局所的な微分の値を乗算していくことで、それぞれの変数の微分の値が得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acde43c",
   "metadata": {},
   "source": [
    "## 加算ノード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb36c11",
   "metadata": {},
   "source": [
    "$L=x+y+z$ について<br>\n",
    "$$\\dfrac{\\partial L}{\\partial x}=1,\\quad \\dfrac{\\partial L}{\\partial y}=1,\\quad \\dfrac{\\partial L}{\\partial z}=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13fafb",
   "metadata": {},
   "source": [
    "上流から伝わった値に、1を乗算して下流に流す。すなわち伝わった微分の値を次のノードへ流すだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70822d83",
   "metadata": {},
   "source": [
    "## 乗算ノード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3c2ec",
   "metadata": {},
   "source": [
    "$L=xyz$ について\n",
    "$$\\dfrac{\\partial L}{\\partial x}=yz,\\quad \\dfrac{\\partial L}{\\partial y}=zx,\\quad \\dfrac{\\partial L}{\\partial z}=xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085cac4",
   "metadata": {},
   "source": [
    "上流から伝わった値に、順伝播の際の入力信号のうち、自分の値を除いた値の積を乗算して下流に流す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecb3d9",
   "metadata": {},
   "source": [
    "# レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187b188",
   "metadata": {},
   "source": [
    "## 逆伝播のイメージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5d1c5",
   "metadata": {},
   "source": [
    "上のイメージをPythonで実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801a0a2",
   "metadata": {},
   "source": [
    "### 加算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b164dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560eb82d",
   "metadata": {},
   "source": [
    "### 乗算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd62cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afe30b",
   "metadata": {},
   "source": [
    "### 行列乗算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3562135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW # Numpy配列上のメモリ位置を固定した上で、要素を上書き\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8ad1c",
   "metadata": {},
   "source": [
    "### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff745b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc19821",
   "metadata": {},
   "source": [
    "## 活性化関数レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7d281",
   "metadata": {},
   "source": [
    "ニューラルネットワークを構成する「層（レイヤ）」を実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82052c",
   "metadata": {},
   "source": [
    "### ReLUレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baaca8b",
   "metadata": {},
   "source": [
    "ReLU（Rectified Linear Unit）\n",
    "$$y=\\begin{cases}\n",
    "    x & (x > 0) \\\\\n",
    "    0 & (x \\leqq 0)\n",
    "  \\end{cases}$$\n",
    "$x$に関する$y$の微分は以下のようになる。\n",
    "$$\\dfrac{\\partial y}{\\partial x}=\\begin{cases}\n",
    "    1 & (x > 0) \\\\\n",
    "    0 & (x \\leqq 0)\n",
    "  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbfe20",
   "metadata": {},
   "source": [
    "順伝播時に、$x$ が0以下であれば、逆伝播では、下流への信号はストップする。<br>\n",
    "つまりReLUレイヤは、回路における「**スイッチ**」のように機能する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "363422e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de478f",
   "metadata": {},
   "source": [
    "逆伝播では、順伝播時に保持した`mask`を使って、逆伝播された`dout`に対し、`mask`の要素が`True`（すなわち負の値）の場所を`0`に設定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51e35e",
   "metadata": {},
   "source": [
    "### Sigmoidレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45d561",
   "metadata": {},
   "source": [
    "$$y=\\dfrac{1}{1+\\exp \\left( -x\\right) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710aeac",
   "metadata": {},
   "source": [
    "step5「 $x$ 」　→　　step4「 $-x$ 」　→　　step3「 $\\exp(-x)$ 」　→　　step2「 $1+\\exp(-x)$ 」　→　　step1「 $\\dfrac{1}{1+\\exp \\left( -x\\right) }(=y)$ 」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afc5df",
   "metadata": {},
   "source": [
    "> **導出**\n",
    "\n",
    ">**step1**\n",
    ">\n",
    ">最終的な値$L$を、シグモイド間数の出力 $y$ で微分する。\n",
    ">$$\\dfrac{\\partial L}{\\partial y}\\tag{1}$$\n",
    "\n",
    ">**step2**\n",
    ">\n",
    ">逆数を返す $b=\\dfrac{1}{a}$ この式の微分は、次の式で表される。\n",
    ">$\\dfrac{\\partial b}{\\partial a} = -a^{-2} =-b^{2}$\n",
    ">\n",
    ">順伝播の出力の2乗に、マイナスを付けた値を乗算。\n",
    ">$$-\\dfrac{\\partial L}{\\partial y}y^{2}\\tag{2}$$\n",
    "\n",
    ">**step3**\n",
    ">\n",
    ">1を加える加算ノードでは、値をそのまま流す。\n",
    ">$$-\\dfrac{\\partial L}{\\partial y}y^{2}\\tag{3}$$\n",
    "\n",
    ">**step4**\n",
    ">\n",
    ">$b=\\exp(a)$ この式の微分は、次の式で表される。$\\dfrac{\\partial b}{\\partial a}=\\exp \\left( a\\right)$\n",
    ">\n",
    ">ネイピア数の、順伝播時の入力の値乗（順伝播時の出力値）を乗算。\n",
    ">$$-\\dfrac{\\partial L}{\\partial y}y^{2}\\exp \\left( -x\\right) \\tag{4}$$\n",
    "\n",
    ">**step5**\n",
    ">\n",
    ">順伝播の際の入力信号のうち、自分の値を除いた値の積（-1）を乗算し、変形。<br><br>\n",
    ">$$\\begin{align}\n",
    "\\dfrac{\\partial L}{\\partial y}y^{2}\\exp \\left( -x\\right)\n",
    "&= \\dfrac{\\partial L}{\\partial y}\\dfrac{1}{\\lbrace(1+\\exp \\left( -x\\right)\\rbrace^{2}} \\exp \\left( -x\\right)\\\\\\\\\n",
    "&= \\dfrac{\\partial L}{\\partial y}\\dfrac{1}{1+\\exp \\left( -x\\right)} \\dfrac{\\exp \\left( -x\\right)}{1+\\exp \\left( -x\\right)}\\\\\\\\\n",
    "&= \\dfrac{\\partial L}{\\partial y}y\\left( 1-y\\right) \\tag{5} \n",
    "\\end{align}$$<br>\n",
    ">Sigmoidレイヤの逆伝播は、順伝播の出力だけ（`x`を使わずに`y`のみ）から、シンプルに計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d76eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e775a",
   "metadata": {},
   "source": [
    "順伝播時の出力を`out`に保持し、逆伝播時に`out`を使って計算する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba029f9d",
   "metadata": {},
   "source": [
    "## Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e4e67",
   "metadata": {},
   "source": [
    "$\\mathrm{X}\\cdot \\mathrm{W}=\\mathrm{Y}$（多次元配列）について\n",
    "$$\\dfrac{\\partial L}{\\partial \\mathrm{X}}=\\dfrac{\\partial L}{\\partial \\mathrm{Y}} \\cdot \\mathrm{W}^\\mathsf{T},\\quad \n",
    "\\dfrac{\\partial L}{\\partial \\mathrm{W}}=\\mathrm{X}^\\mathsf{T} \\cdot \\dfrac{\\partial L}{\\partial \\mathrm{Y}}$$\n",
    "$\\mathsf{T}$は転置を表し、次元数を揃えるために転置を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9f0c2a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47313e",
   "metadata": {},
   "source": [
    "## Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c170b",
   "metadata": {},
   "source": [
    "ここでは、交差エントロピー誤差も含めて実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc01e6",
   "metadata": {},
   "source": [
    "![](image/06_softmax-with-loss.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4cc7b0",
   "metadata": {},
   "source": [
    "分類問題において<br>\n",
    "「**ソフトマックス関数**」の損失関数に「**交差エントロピー誤差**」を用いると、逆伝播が $(y_1-t_1, \\quad y_2-t_2, \\quad y_3-t_3)$ とキレイな結果になる。<br><br>\n",
    "回帰問題において<br>\n",
    "「**恒等関数**」の損失関数に「**二乗和誤差**」を用いると、逆伝播が $(y_1-t_1,\\quad  y_2-t_2, \\quad y_3-t_3)$ とキレイな結果になる。<br><br>\n",
    "これらは偶然ではなく、逆伝播がキレイな値になるよう誤差関数が定義されたものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23aa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37bb2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e1f70",
   "metadata": {},
   "source": [
    "教師ラベル：$(0, 1, 0)$<br>\n",
    "1. Softmaxレイヤの出力：$(0.3, 0.2, 0.5)$<br>\n",
    "$(0.3, -0.8, 0.5)$という大きな誤差を伝播するため、これより前のレイヤが学習する内容は大きくなる。<br><br>\n",
    "2. Softmaxレイヤの出力：$(0.01, 0.99, 0)$<br>\n",
    "$(0.01, -0.01, 0)$という小さな誤差を伝播するため、これより前のレイヤが学習する内容は大きくなる。<br><br>\n",
    "誤差の大きさによって、学習する量が変わるところが特徴。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e24b1e",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651a1dd",
   "metadata": {},
   "source": [
    "レイヤを組み合わせることで、ニューラルネットワークを構築できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab72c9f",
   "metadata": {},
   "source": [
    "**ニューラルネットワークの全体像**\n",
    ">1. ミニバッチ<br>\n",
    ">訓練データの中からランダムに一部のデータを選び出す。<br><br>\n",
    ">2. 勾配の算出（**誤差逆伝播法が活躍**）<br>\n",
    ">各重みパラメータに関する損失関数の勾配を求める。<br><br>\n",
    ">3. パラメータの更新<br>\n",
    ">重みパラメータを勾配方向に微少量だけ更新する。<br><br>\n",
    ">4. 繰り返し<br>\n",
    ">1〜3を繰り返す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d624d7",
   "metadata": {},
   "source": [
    "**TwoLayerNetクラスのメソッド**\n",
    "\n",
    "| メソッド | 説明 | 引数 |\n",
    "|:------|:----|:----|\n",
    "| `init(self, input_size, hidden_size, output_size)` |初期化|入力ニューロン数, 中間ニューロン数, 出力ニューロン数|\n",
    "| `predict(self, x)` |認識（推論）|画像データ|\n",
    "| `loss(self, x, t`) |損失関数|画像データ, 正解ラベル|\n",
    "| `accuracy(self, x, t)` |認識精度|画像データ, 正解ラベル|\n",
    "| `numerical_gradient(self, x, t)` |重みパラメータに対する勾配を求める|画像データ, 正解ラベル|\n",
    "| `gradient(self, x, t)` |誤差逆伝播法|画像データ, 正解ラベル|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24320407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a9230e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88648310",
   "metadata": {},
   "source": [
    "## 勾配確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9126e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.782084355307434e-10\n",
      "b1:2.8926422388528098e-09\n",
      "W2:5.5476883095377905e-09\n",
      "b2:1.40230064005209e-07\n"
     ]
    }
   ],
   "source": [
    "from source.dataset01.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 各重みの絶対誤差の平均を求める\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd11ea",
   "metadata": {},
   "source": [
    "数値微分と誤差逆伝播法でそれぞれ求めた勾配の差はかなり小さいことから、誤差逆伝播法の実装に誤りがないことになる。<br>\n",
    "（コンピュータでは、有限の精度で計算（例えば32ビットの浮動小数点）が行われるため、誤差は0にならない。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c527da",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58422089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10033333333333333, 0.1019\n",
      "train acc, test acc | 0.9057, 0.9079\n",
      "train acc, test acc | 0.9226833333333333, 0.9256\n",
      "train acc, test acc | 0.9375333333333333, 0.9387\n",
      "train acc, test acc | 0.94505, 0.9432\n",
      "train acc, test acc | 0.95, 0.9497\n",
      "train acc, test acc | 0.9559833333333333, 0.9543\n",
      "train acc, test acc | 0.9582166666666667, 0.9558\n",
      "train acc, test acc | 0.9619666666666666, 0.9596\n",
      "train acc, test acc | 0.9660666666666666, 0.9613\n",
      "train acc, test acc | 0.9670166666666666, 0.9622\n",
      "train acc, test acc | 0.9700333333333333, 0.9645\n",
      "train acc, test acc | 0.9727, 0.9656\n",
      "train acc, test acc | 0.97325, 0.9669\n",
      "train acc, test acc | 0.9761833333333333, 0.9694\n",
      "train acc, test acc | 0.9761166666666666, 0.9686\n",
      "train acc, test acc | 0.9776833333333333, 0.9701\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6e0d9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAArMElEQVR4nO3de3xU9Z3/8ddnZnK/QsI9IEjxXgVF1/tqrRfUeq23qu26rei2um23umq33tr+XFfbbn/+tCp1tVatVuvd4n2tbtd6AUURUUEECdckkEAuk8nMfH5/zIAhhDCBTE7IvJ+PRx7JOec757wnhPOZc/l+j7k7IiKSu0JBBxARkWCpEIiI5DgVAhGRHKdCICKS41QIRERynAqBiEiOy1ohMLO7zWy1mX2wheVmZreY2UIze9/M9s1WFhER2bJsHhH8Djiuh+XTgEnpr+nA7VnMIiIiW5C1QuDurwFremhyMvB7T3kDqDSzUdnKIyIi3YsEuO0xwNJO07XpeSu6NjSz6aSOGigpKdlvt91265eAIiKDxezZs+vdfVh3y4IsBNbNvG7Hu3D3GcAMgKlTp/qsWbOymUtEZNAxsyVbWhbkXUO1wNhO0zXA8oCyiIjkrCALwVPAN9N3Dx0INLn7ZqeFREQku7J2asjMHgSOAKrNrBa4FsgDcPc7gJnA8cBCoBW4IFtZRES6Siad9niSaEdik+/t8QTJbk5Sd3cu26xrm81bJdyJJ5LEk0484XQkkyQSTjzZaV4iSSLpdCSdRLptR8JJJJN0dGp74IQqjtxteN/8AjrJWiFw93O2styB72Vr+yKSfe6pHVZ7fMNONEksvTNt70hu3LHG4l/83N6RJJZI0t6RJOFO0h331LqSDu6k5rFhnqfnpaad1E7codMy37iNrjv2aEei08+bZthRhENGOGTkhUI7ViEQkW2XTDrxZGonmEh6aoeZTO0oE53mJ91JJlOfOjdOb/i50/yNO+qOBNGOL3aO0XhqhxyNp6c70m3SO8vU/E13phvWs+HnvueESRIhARYibnmELUmVrSdCkjxzIpYgz5I0WRnrKKPQOtgr9DklkTgV4SQl4TjFoQTLiybRWD6WYdbEQS0vUxiKU2hxCuigwDr4dOSJrK/ai+FtnzH509vI8w7CyRhmBhbis70uZV31vpSumcv4ebfhFgYL4RhuYRbvfjEtlbtQ3vAeYz79I0kLASHcQmAhFu/6beJlNQypn8Wozx4jTIKQJwh5nJAnqD/sZ1A+mvJFf6Zs7j1YMrXckh2YJ0ic9zjh0uGE3rwdlvwvHPtAFn7fKgQi28U9dXqhNZagNRanNZagpb3L91ic1vb09/T8ttim013bRTs238FGiBMhQZgkIZwQqTZNlAIwlHUU0U7IkhvbdBBmqY8AYDf7nEprJp8O8oiTT5z1FPPX5JfJCxtnRP7KqPA6ikIJCkNxikJx6vPG8D8VJ1JZlMf5a29jSGQNkbwkkaIkEUuyomIf3ptwIQWREMe9dymF8fWE2LD9JGvHfpVV+/0LBZEQuz1yJJaMEfIk5gnwJInJ55M48idYvJX8X0yEZDy1bIPDLoOjrobmOvjFlzb/B/jqdXDoD2HNIrjlXEh0WX7IL+CA02DlXLjjN1/MDxdApIApBx0NX54Ayxvh42UQyYf8gtQ5n2SMoTVlMH44LCmC91aBJ1NfyVT+kTUFMHYEfBSHN1//Ynn6/Y09ajqMGg7vt8Dq/4VQBMKR1PdQhPIhERhaCqvyIByCvPyNywjnEQpHIGQwZCeINW/HX2rPbEd7QpluH5WeuDuxROq8aiyepCOROlURS3/vOh2LJ2nrSNAWS9AaS9DWkaC1vYOOaAuJ9hYS0RZW+FCa41DWVsuI6GKso5VwvJVIvJVIso274sfTTj7Hh97gqPC76Z11gjwSREgwveNfSBDm2+GZfC3yBgWWJN/i5FvqE++PR/8XxflhzltzK1ObXiRMnLDHMU8Si5Rx72GvEA4ZX33/MsbXvbzJ+20pGs2zX32RcAgOef1Chte9vsny5opJzDp+JvmREHs/93VK697dZHli9H7wnZcJhwx+cxCs/jC9xCBSABOPgnP+kJp1zwnQUgfhPLAQhMKw8xGpnTHAg9+AjhawcGqZpZcfeHFq+eP/BHh6eSj1fcLhsNdpkIjDf/8svRMMf/F97N/B+EOhIwpz7k+/9osdKSO/DMN3g442WPzXVOZwQWqHHi6A8lFQNCS14461pJfnb35yPweY2Wx3n9rtMhUCCVoy6ayPxmlsi9HUGmPd+maamxuJrm8k2tLIcq9mRawY1q/gS03/i7U3Y8kOSMYhGecZP4SFiVHslFjCmfYSkfQOOGJJwiS4NX4Kn/oYDgx9yEXhp9PLk4QttbP+UcfFLPLRnBl+hWsjv6fE2jfJ94/lv6WpqIYzoo9ydtNdm+W/56DnsdIRTFl6H5OW/AHCeVinr8ZznqG4uISS9+4h9MmzEMpL7UxDkdSO6bQZqRW9/zAsm73x0yAWhvxiOOxHqeXzn4aGhZ12tCEoKIcp56aWL3gJ1q/4YiccCkNhBUw6OrW8dnZqRx0uSK0/UgAFZVA5LrU8ui71mnBBeh25t7MczFQIJKviiSQt7QnWRTtobovS1lRPbH0dHc1rSLQ0sCI8hqXhsXSsX83By35HqKOZcEcLefEW8pOtzOiYxrOJA9jTFvNE/tXk2abH9/+avITXCr/Cofkf84vmqzbb/gM738iioX/Prs1v8bWF1+AWTn2FIngowltT/p3mYfsyuu5/2WX+LRCKYOkdsYXzWHPYT8kbsStldbMpWvhnwgUlWH4J5JVAfgnsdgIUVcK6FakdbX4J5BV/8T1SoJ2mDHgqBLJVsXiSta0x6pvbaWiO0dAcpampiY7mBtbHYEWykpZoOwc0PElerJHCjiaK402UJtfxXHxfHkh8lUrWM6fwos3W/auOr3Obn86kwnU8mvwBbaFiYqFiYpFSEpESPqg5m/qarzIi1MSen/+BvOIK8kvKKSytpKi0grya/aBiTOr0QNva1A44nP/FKQrthEW2SoUgl0TXQbSJZHQ9Lc2NrGtcS2M8jyUlX6ahuZ0xC+4n3LwCb1+PtTcTjjfzXnw8N0dPBuCF/MsZaWsoIUrYUn8bj/sR3FT4z5Tmh5m5/nTyiNMWKqE1XE57XiWfjDyBT3c+j/ICY/KiGYRKqgiXVpFfVk1hWTVFw3emsGJY6k4MEQlET4VAdw3tiDraYM0iEvULaFo6n/rmGM8P+QafrG7mRwu+yfjk54SAsvRXbXI3vhu7BoCX8+9nXKiOViuiPVRMR7iE0LAJ/Mtuu1BVmk/ewmNoizjJkgoKSisoKK3m1BG7c+rYA1Lbbv4ICisoiuRTlI4zmlTPQQD2v6EffxEi0hd0RDBQJRPQtBQaFpJct4Jl40/nk1XrqfnLD5m0aiahTuPzfZAcz4mxG6gZUsSZpXMYk99GXnEFhSUVFJVVUDxkFCWjd6WqpIAhBRDJLwjwjYlIEHREMJC1N8OqeVCzP27Guv+5g8js/6Jg/RIiyRgA7sZX2svpIMIpoZ3Ys+gsYpU7kz9iF6rG7cHOY0Ywb3gpJQUR4CvBvh8R2eGoEPS3tYvhk+dJ1L5DbOlsChsXYjgXVd/L6/VFHNvxGceGylnkR7M6fxw+dCJFo3bj+jHj2HVUGV8afgwVRXlBvwsRGURUCLKloy31SX/5u3QsfYf548/l7bYxROY/zreWXU+DV/J+cgJzk6fxWd6XaA1VcPLkanYZ8V3KRpRxxogyhpbkB/0uRCQHqBD0hXg7xKNQWMHapfPJe/QCipo+IZzuKt/k5dwyexQvJfdjfOkk3qt5iJpxO7PH6ArOGFPOmMoi3VEjIoFRIdgW8Rj+/h9p/ewtErXvUNL4ETMrzubf205jXdMafpMX4X0/keXFu+GjpjBm3ETOrankhtHlDC8rDDq9iMgmVAi2wfJZTzL6uUuIezFzkxOY69P4uH0P9p8wlL1GTyAy+gnOH11BRbHO5YvIwKdCsA0+rY+yOjmRWYf+lim7TuCbI8vTd+yIiOx4tPfaBu8UHsh/xn7GJ1+ZQn4kyKd9iohsP+3FtsHKdW1UlxaoCIjIoKAjgm1wxoIr+PtwGfDVoKOIiGw3faTdBqPbF1KR1/VRSCIiOyYVgt5KJqlKNhAvHhl0EhGRPqFC0EttjavII4GXjw46iohIn1Ah6KU1KxcDkDekJtggIiJ9RBeLe6muNcl7iQMYNXKXoKOIiPQJHRH00qLQTny34wdU7rR30FFERPqECkEvrWhsA2BkucYMEpHBQaeGeungD67m2cKPKMo/IegoIiJ9QkcEvVTYupJkWEcDIjJ4qBD0UllsNS0Fw4KOISLSZ1QIesOdqmQ9MXUmE5FBRIWgF2LNayminWSZOpOJyOChi8W9UL++lefixzFu5H5BRxER6TM6IuiF5bFifhr/JnkTDgo6iohIn1Eh6IXVDQ0UEGNUhe4aEpHBQ4WgF0bM/S0fFlzAyNJw0FFERPpMVguBmR1nZh+b2UIzu7Kb5RVm9rSZvWdm88zsgmzm2V62fjlrqKCsuCjoKCIifSZrhcDMwsBtwDRgD+AcM9ujS7PvAR+6+z7AEcAvzSw/W5m2V0HbKhrC1ZhZ0FFERPpMNo8IDgAWuvsid48BDwEnd2njQJml9qylwBognsVM26W0fRXN+epMJiKDSzYLwRhgaafp2vS8zm4FdgeWA3OB77t7suuKzGy6mc0ys1l1dXXZyrtVQxL1RIvUmUxEBpdsFoLuzp94l+ljgTnAaGAycKuZlW/2IvcZ7j7V3acOGxbMJ/JEIsmt8VNYPvKoQLYvIpIt2SwEtcDYTtM1pD75d3YB8JinLAQ+A3bLYqZtVt8SY0b8BGI7HR50FBGRPpXNQvA2MMnMJqQvAJ8NPNWlzefAUQBmNgLYFViUxUzbbPXqVYyzVYwuU2dsERlcslYI3D0OXAI8D8wHHnb3eWZ2sZldnG72M+BgM5sLvAxc4e712cq0PfzjZ3mt4IfU2ICMJyKyzbL68dbdZwIzu8y7o9PPy4Fjspmhr3SsTV33rhq1U8BJRET6lnoWZ2rdCtZ6KUMrK4JOIiLSp1QIMpTfupKGkDqTicjgo0KQoZLoKtapM5mIDEK6BSZDvw2fSc2woewbdBARkT6mI4IMuDuPtuzNutGHBh1FRKTPqRBkYG3jWvZNfsC44ljQUURE+pwKQQaaPpvNQ/k/Z5f4J0FHERHpcyoEGWip+xyA0uHqQyAig48KQQZia2oBGDpyQsBJRET6ngpBBnzdcpq9kOrq6qCjiIj0ORWCDERaVlIfqiIcUmcyERl81I8gAw8WnUMk1MjPgw4iIpIFOiLIwNutI1kzbP+gY4iIZIUKwdYkE0xtepFdCxqCTiIikhUqBFvR3LCM/wjdyr4d7wYdRUQkK1QItmLNis8AyB9SE3ASEZHsUCHYivV1qQfSlAxTZzIRGZxUCLYi1pAqBENGjAs4iYhIdqgQbEWiaRntHmHYiDFBRxERyQoVgq14ruJMvh35P+TnhYOOIiKSFSoEW/FpSyGNlXsGHUNEJGtUCLZiv9WPcmj+wqBjiIhkjQpBT9yZ3nYXhyTeDjqJiEjWqBD0INq0mnziePnooKOIiGSNCkEPNnQmy6vUHUMiMnipEPSgaXXqyWQl1epDICKDlwpBD6LpzmQVI9WrWEQGLxWCHrxReQIHRf8fw0bpiEBEBi8Vgh6sWB+ntWgkxQX5QUcREckaPaGsB7t//gfKCvOBY4KOIiKSNSoEPTi08UlW5E8IOoaISFbp1NCWuFOVqCdWPDLoJCIiWaVCsAWxlkaKiZIsGxV0FBGRrFIh2II1KxcDENGTyURkkMtqITCz48zsYzNbaGZXbqHNEWY2x8zmmdmr2czTG011K0i4UTRUhUBEBresFQIzCwO3AdOAPYBzzGyPLm0qgd8AJ7n7nsAZ2crTWwtKJrNL++8pnnhI0FFERLIqm0cEBwAL3X2Ru8eAh4CTu7T5BvCYu38O4O6rs5inV1Y2RUkQZuSQkqCjiIhkVTYLwRhgaafp2vS8znYBhpjZX8xstpl9s7sVmdl0M5tlZrPq6uqyFHdTYxbcz48LHqa8UHfYisjgls29nHUzz7vZ/n7AUUAR8Dcze8PdP9nkRe4zgBkAU6dO7bqOrBhX/1cmhusx6+5tiIgMHhkdEZjZo2Z2gpn15giiFhjbaboGWN5Nm+fcvcXd64HXgH16sY2sKWlfxbr8YUHHEBHJukx37LeTOp+/wMxuNLPdMnjN28AkM5tgZvnA2cBTXdo8CRxmZhEzKwb+DpifYaasGpKoJ1qkzmQiMvhldGrI3V8CXjKzCuAc4EUzWwr8Frjf3Tu6eU3czC4BngfCwN3uPs/MLk4vv8Pd55vZc8D7QBK4y90/6JN3th0S7S1U0EyyVJ3JRGTwy/gagZlVAecB5wPvAg8AhwLfAo7o7jXuPhOY2WXeHV2mbwZu7k3obFtbv5KYDyU0RMNPi8jgl+k1gseA/wGKga+5+0nu/kd3vxQozWbAICxLVnFw+61Edzs96CgiIlmX6RHBre7+390tcPepfZhnQFjRFAVgZEVhwElERLIv04vFu6d7AQNgZkPM7LvZiRS80vkPcVfezYwsDQcdRUQk6zItBBe6e+OGCXdfC1yYlUQDQFH9XPYPfcLQMvUqFpHBL9NCELJOPavS4wgN2uc35rWspD5URSikzmQiMvhleo3geeBhM7uDVO/gi4HnspYqYCXtq1iXNzzoGCIi/SLTQnAFcBHwT6SGjngBuCtboYJWGa9nVfkuQccQEekXmXYoS5LqXXx7duMEz5MJFieH01KhQiAiuSHTfgSTzOxPZvahmS3a8JXtcEFobEtwevu1LN3lW0FHERHpF5leLL6H1NFAHDgS+D1wX7ZCBWlDH4JR6kMgIjki00JQ5O4vA+buS9z9OuAr2YsVnPj8Z5iZfxVjw/VBRxER6ReZXiyOpoegXpAeSG4ZMChvq+mo+5S9Q0tYVTUo356IyGYyPSL4Aalxhv6Z1INkziM12Nyg403LaPUCqqqqg44iItIvtnpEkO48dqa7Xw40AxdkPVWA8lpWUmdV7BTR8BIikhu2ekTg7glgv849iwez4ugqmvL0ZDIRyR2ZXiN4F3jSzB4BWjbMdPfHspIqQPN8PKGyEewddBARkX6SaSEYCjSw6Z1CDgy6QvCT2D9wxoQaTgo6iIhIP8m0Z/Ggvi6wwfpoB83tcUaWqw+BiOSOjAqBmd1D6ghgE+7+j32eKECNC9/knYLpzI/9P2Bi0HFERPpFpqeGnun0cyFwKrC87+MEa/3qzxlrzVQM0a2jIpI7Mj019GjnaTN7EHgpK4kCFG1YCsCQkeODDSIi0o8y7VDW1SRgXF8GGQiSTcto9wjVI0YFHUVEpN9keo1gPZteI1hJ6hkFg0q4eQX1NpQxeXlBRxER6TeZnhoqy3aQgWBuaFfyCodyTtBBRET6UabPIzjVzCo6TVea2SlZSxWQB/1YXh71naBjiIj0q0yvEVzr7k0bJty9Ebg2K4mC4s7axkZG6jkEIpJjMi0E3bXL9NbTHUJ0XT1v+Hkc3fx00FFERPpVpoVglpn9yswmmtnOZvafwOxsButvDSs+AyC/YkTASURE+lemheBSIAb8EXgYaAO+l61QQVi3+nMAiqsH3V2xIiI9yvSuoRbgyixnCdSGzmQVI3cKOImISP/K9K6hF82sstP0EDN7PmupApBoXEbCjWEjdUQgIrkl01ND1ek7hQBw97UMsmcWfxjZnTvt65QU6a4hEcktmRaCpJlt/KhsZuPpZjTSHdlffR+eqDg/6BgiIv0u01tA/w34q5m9mp4+HJienUjBSK5dwrgyPaJSRHJPpheLnzOzqaR2/nOAJ0ndOTRo/Hrt95gTOoFUjRMRyR2ZXiz+DvAy8KP0133AdRm87jgz+9jMFprZFu86MrP9zSxhZl/PLHbf6mhtpJQ2kqWjg9i8iEigMr1G8H1gf2CJux8JTAHqenqBmYWB24BpwB7AOWa2xxba/QcQ2F1Ia1YuASCvUoVARHJPpoUg6u5RADMrcPePgF238poDgIXuvsjdY8BDwMndtLsUeBRYnWGWPte0cjEAhdVjg4ogIhKYTAtBbbofwRPAi2b2JFt/VOUYYGnndaTnbWRmY0g99vKOnlZkZtPNbJaZzaqr6/FAZJu01qdilo9QZzIRyT2ZXiw+Nf3jdWb2ClABPLeVl1l3q+oy/WvgCndPmHXXfOP2ZwAzAKZOndrnt60uzNuVpzrO5/sjJ/T1qkVEBrxejyDq7q9uvRWQOgLofK6lhs2PIqYCD6WLQDVwvJnF3f2J3ubaHh8lxvBg6ESuLivpz82KiAwI2RxK+m1gkplNAJYBZwPf6NzA3Td+BDez3wHP9HcRALC6eUwuS9LTUYmIyGCVtULg7nEzu4TU3UBh4G53n2dmF6eX93hdoD+ds+wGGiPDgLOCjiIi0u+y+nAZd58JzOwyr9sC4O7/kM0sPRkSr2dF6V5BbV5EJFCZ3jU0aCVjbQxhHYky9SEQkdyU84Vg7apUZ7JwhQqBiOSmnC8EjRs6k1WpM5mI5KacLwRLwuO4OPYDisftG3QUEZFA5HwhWBot5rnkAVQPHxV0FBGRQOR8IWDZbA6OfERVSX7QSUREApHV20d3BFM+v4fD8xYTCv0o6CgiIoHI+SOC4ugqmvL0ZDIRyV05Xwgq4vW0FY0MOoaISGByuhB4PEZVci3xkhFBRxERCUxOF4KmumWEzLHyMVtvLCIySOV0IVgRL+Xk9p8SnXhs0FFERAKT04VgZYvznn+JoSPUq1hEcldOF4LYkrf5evhVRpXlBR1FRCQwOV0Iqpb8mZ9F7mFYWVHQUUREApPThSDSvII6qyISCQcdRUQkMDldCIrUmUxEJLcLQXlHHS0Fw4OOISISqNwtBMkk1ckGOko06qiI5LacLQTr2+Mc0n4Li770raCjiIgEKmcLwar17axmCJXD9IhKEcltOVsI1i1+l0vCj1OT3xZ0FBGRQOVsIeDzN7ks7xFGlOb8IxlEJMflbCFINi2jw8NUj9SAcyKS23K2EISal1NnQynM1/ASIpLbcrYQFLatZm1YnclERHK2EJTG6mkpUCEQEcnZK6Wn+s2ctPNQDgg6iIhIwHLyiCDakaChLUnV0KFBRxERCVxOFoKGpZ9wQ+QuJoWWBR1FRCRwOVkI1i/7kG9E/puR+e1BRxERCVxOFoK2+qUAVIwYH2wQEZEBICcLQbxxGUk3qkeNCzqKiEjgcrIQhJtX0GAVlBbrEZUiIlktBGZ2nJl9bGYLzezKbpafa2bvp79eN7N9splng1h7G6vDI/tjUyIiA17W+hGYWRi4DTgaqAXeNrOn3P3DTs0+A/7e3dea2TRgBvB32cq0wb8X/pDyygj3ZXtDIiI7gGweERwALHT3Re4eAx4CTu7cwN1fd/e16ck3gJos5tloRVOUUZU6LSQiAtktBGOApZ2ma9PztuTbwLPdLTCz6WY2y8xm1dXVbVeojrb1/Ef0pxyYfHe71iMiMlhksxBYN/O824ZmR5IqBFd0t9zdZ7j7VHefOmzY9o0PtGbFYo4Mv8fIvNbtWo+IyGCRzUJQC4ztNF0DLO/ayMz2Bu4CTnb3hizmAaBp1RIAiqrHbqWliEhuyGYheBuYZGYTzCwfOBt4qnMDMxsHPAac7+6fZDHLRq31nwNQNkx9CEREIIt3Dbl73MwuAZ4HwsDd7j7PzC5OL78DuAaoAn5jZgBxd5+arUwA8bW1AFSPGp/NzYiI7DCyOgy1u88EZnaZd0enn78DfCebGbpqjBnzfSd2Ky/vz82KiAxYOfc8gieKT2de2dG8Yt1dyxaRgaCjo4Pa2lqi0WjQUXY4hYWF1NTUkJeX+WN4c64QrGyKMrK8MOgYItKD2tpaysrKGD9+PKYPbRlzdxoaGqitrWXChAkZvy7nxhr68erLODPxTNAxRKQH0WiUqqoqFYFeMjOqqqp6fSSVU4UgGYuyr8+jOk/PIRAZ6FQEts22/N5yqhCsWZ3qQxCuGB1wEhGRgSOnCkHjylQhKBiqzmQismWNjY385je/2abXHn/88TQ2NvZtoCzLqULQWpca+qhs+E4BJxGRgaynQpBIJHp87cyZM6msrMxCquzJqbuG6mNh3kruys6jMr+aLiLBuv7peXy4fF2frnOP0eVc+7U9t7j8yiuv5NNPP2Xy5MkcffTRnHDCCVx//fWMGjWKOXPm8OGHH3LKKaewdOlSotEo3//+95k+fToA48ePZ9asWTQ3NzNt2jQOPfRQXn/9dcaMGcOTTz5JUdGmIx8//fTT/PznPycWi1FVVcUDDzzAiBEjaG5u5tJLL2XWrFmYGddeey2nn346zz33HD/+8Y9JJBJUV1fz8ssvb/fvI6cKwdsFB3JXYjgfD60KOoqIDGA33ngjH3zwAXPmzAHgL3/5C2+99RYffPDBxtsy7777boYOHUpbWxv7778/p59+OlVVm+5bFixYwIMPPshvf/tbzjzzTB599FHOO++8TdoceuihvPHGG5gZd911FzfddBO//OUv+dnPfkZFRQVz584FYO3atdTV1XHhhRfy2muvMWHCBNasWdMn7zenCsHKpijDywoJhXQ3gsiOoqdP7v3pgAMO2OTe/FtuuYXHH38cgKVLl7JgwYLNCsGECROYPHkyAPvttx+LFy/ebL21tbWcddZZrFixglgstnEbL730Eg899NDGdkOGDOHpp5/m8MMP39hm6NChffLecuoawT8s+iHX251BxxCRHVBJScnGn//yl7/w0ksv8be//Y333nuPKVOmdHvvfkFBwcafw+Ew8Xh8szaXXnopl1xyCXPnzuXOO+/cuB533+xW0O7m9YWcKgSjY0sozbzXtYjkqLKyMtavX7/F5U1NTQwZMoTi4mI++ugj3njjjW3eVlNTE2PGpJ7Zde+9926cf8wxx3DrrbdunF67di0HHXQQr776Kp999hlAn50ayplC4IkOhibXEC/RQ+tFpGdVVVUccsgh7LXXXlx++eWbLT/uuOOIx+PsvffeXH311Rx44IHbvK3rrruOM844g8MOO4zq6uqN83/yk5+wdu1a9tprL/bZZx9eeeUVhg0bxowZMzjttNPYZ599OOuss7Z5u52Ze7cPDRuwpk6d6rNmzer165pWLaHi9r35667/xqHn/GsWkolIX5k/fz6777570DF2WN39/sxs9paG+c+ZI4I1KxYDUDC0JtggIiIDTM7cNVQXDTE3cRA7j9o16CgiIgNKzhwRxIftzh/GXsuw8QPjVjQRkYEiZ44IDp5YzcETq7feUEQkx+TMEYGIiHRPhUBEJMepEIiIdLE9w1AD/PrXv6a1tbUPE2WXCoGISBe5Vghy5mKxiOzA7jlh83l7ngIHXAixVnjgjM2XT/4GTDkXWhrg4W9uuuyCP/e4ua7DUN98883cfPPNPPzww7S3t3Pqqady/fXX09LSwplnnkltbS2JRIKrr76aVatWsXz5co488kiqq6t55ZVXNln3T3/6U55++mna2to4+OCDufPOOzEzFi5cyMUXX0xdXR3hcJhHHnmEiRMnctNNN3HfffcRCoWYNm0aN954Yy9/eVunQiAi0kXXYahfeOEFFixYwFtvvYW7c9JJJ/Haa69RV1fH6NGj+fOfU4WlqamJiooKfvWrX/HKK69sMmTEBpdccgnXXHMNAOeffz7PPPMMX/va1zj33HO58sorOfXUU4lGoySTSZ599lmeeOIJ3nzzTYqLi/tsbKGuVAhEZODr6RN8fnHPy0uqtnoEsDUvvPACL7zwAlOmTAGgubmZBQsWcNhhh3HZZZdxxRVXcOKJJ3LYYYdtdV2vvPIKN910E62traxZs4Y999yTI444gmXLlnHqqacCUFhYCKSGor7gggsoLi4G+m7Y6a5UCEREtsLdueqqq7jooos2WzZ79mxmzpzJVVddxTHHHLPx0353otEo3/3ud5k1axZjx47luuuuIxqNsqUx37I17HRXulgsItJF12Gojz32WO6++26am5sBWLZsGatXr2b58uUUFxdz3nnncdlll/HOO+90+/oNNjxroLq6mubmZv70pz8BUF5eTk1NDU888QQA7e3ttLa2cswxx3D33XdvvPCsU0MiIv2k8zDU06ZN4+abb2b+/PkcdNBBAJSWlnL//fezcOFCLr/8ckKhEHl5edx+++0ATJ8+nWnTpjFq1KhNLhZXVlZy4YUX8uUvf5nx48ez//77b1x23333cdFFF3HNNdeQl5fHI488wnHHHcecOXOYOnUq+fn5HH/88dxwww19/n5zZhhqEdlxaBjq7aNhqEVEpFdUCEREcpwKgYgMSDvaaeuBYlt+byoEIjLgFBYW0tDQoGLQS+5OQ0PDxn4ImdJdQyIy4NTU1FBbW0tdXV3QUXY4hYWF1NT07pG8KgQiMuDk5eUxYcKEoGPkjKyeGjKz48zsYzNbaGZXdrPczOyW9PL3zWzfbOYREZHNZa0QmFkYuA2YBuwBnGNme3RpNg2YlP6aDtyerTwiItK9bB4RHAAsdPdF7h4DHgJO7tLmZOD3nvIGUGlmo7KYSUREusjmNYIxwNJO07XA32XQZgywonMjM5tO6ogBoNnMPt7GTNVA/Ta+NpsGai4YuNmUq3eUq3cGY66dtrQgm4WguyHzut4Llkkb3H0GMGO7A5nN2lIX6yAN1FwwcLMpV+8oV+/kWq5snhqqBcZ2mq4Blm9DGxERyaJsFoK3gUlmNsHM8oGzgae6tHkK+Gb67qEDgSZ3X9F1RSIikj1ZOzXk7nEzuwR4HggDd7v7PDO7OL38DmAmcDywEGgFLshWnrTtPr2UJQM1FwzcbMrVO8rVOzmVa4cbhlpERPqWxhoSEclxKgQiIjkuZwrB1oa7CIKZjTWzV8xsvpnNM7PvB52pMzMLm9m7ZvZM0Fk2MLNKM/uTmX2U/r0dFHQmADP7Yfrf8AMze9DMejf8Y9/luNvMVpvZB53mDTWzF81sQfr7kAGS6+b0v+P7Zva4mVUOhFydll1mZm5m1f2dq6dsZnZpel82z8xu6ott5UQhyHC4iyDEgR+5++7AgcD3BkiuDb4PzA86RBf/F3jO3XcD9mEA5DOzMcA/A1PdfS9SN0ecHVCc3wHHdZl3JfCyu08CXk5P97ffsXmuF4G93H1v4BPgqv4ORfe5MLOxwNHA5/0dqJPf0SWbmR1JakSGvd19T+AXfbGhnCgEZDbcRb9z9xXu/k765/Wkdmpjgk2VYmY1wAnAXUFn2cDMyoHDgf8CcPeYuzcGGuoLEaDIzCJAMQH1h3H314A1XWafDNyb/vle4JT+zATd53L3F9w9np58g1Q/osBzpf0n8K9008G1v2wh2z8BN7p7e7rN6r7YVq4Ugi0NZTFgmNl4YArwZsBRNvg1qf8IyYBzdLYzUAfckz5ldZeZlQQdyt2Xkfpk9jmp4VGa3P2FYFNtYsSG/jnp78MDztOdfwSeDToEgJmdBCxz9/eCztKNXYDDzOxNM3vVzPbvi5XmSiHIaCiLoJhZKfAo8AN3XzcA8pwIrHb32UFn6SIC7Avc7u5TgBaCOc2xifQ595OBCcBooMTMzgs21Y7DzP6N1GnSBwZAlmLg34Brgs6yBRFgCKlTyZcDD5tZd/u3XsmVQjBgh7IwszxSReABd38s6DxphwAnmdliUqfRvmJm9wcbCUj9O9a6+4ajpj+RKgxB+yrwmbvXuXsH8BhwcMCZOlu1YVTf9Pc+OZ3QF8zsW8CJwLk+MDo1TSRV0N9L//3XAO+Y2chAU32hFngsPWLzW6SO2Lf7YnauFIJMhrvod+lK/l/AfHf/VdB5NnD3q9y9xt3Hk/pd/be7B/4J191XAkvNbNf0rKOADwOMtMHnwIFmVpz+Nz2KAXARu5OngG+lf/4W8GSAWTYys+OAK4CT3L016DwA7j7X3Ye7+/j0338tsG/6b28geAL4CoCZ7QLk0wejpOZEIUhfkNow3MV84GF3nxdsKiD1yft8Up+456S/jg861AB3KfCAmb0PTAZuCDYOpI9Q/gS8A8wl9f8qkCEKzOxB4G/ArmZWa2bfBm4EjjazBaTuhLlxgOS6FSgDXkz/7d8xQHINCFvIdjewc/qW0oeAb/XFkZSGmBARyXE5cUQgIiJbpkIgIpLjVAhERHKcCoGISI5TIRARyXEqBCJZZmZHDKQRXEW6UiEQEclxKgQiaWZ2npm9le7cdGf6eQzNZvZLM3vHzF42s2HptpPN7I1OY+kPSc//kpm9ZGbvpV8zMb360k7PUXhgw/gwZnajmX2YXk+fDCks0lsqBCKAme0OnAUc4u6TgQRwLlACvOPu+wKvAtemX/J74Ir0WPpzO81/ALjN3fchNd7QivT8KcAPSD0PY2fgEDMbCpwK7Jlez8+z+R5FtkSFQCTlKGA/4G0zm5Oe3pnUoF5/TLe5HzjUzCqASnd/NT3/XuBwMysDxrj74wDuHu00hs5b7l7r7klgDjAeWAdEgbvM7DRgQIy3I7lHhUAkxYB73X1y+mtXd7+um3Y9jcnS03DA7Z1+TgCR9BhYB5AaffYU4LneRRbpGyoEIikvA183s+Gw8Tm/O5H6P/L1dJtvAH919yZgrZkdlp5/PvBq+lkStWZ2SnodBenx7buVfg5FhbvPJHXaaHKfvyuRDESCDiAyELj7h2b2E+AFMwsBHcD3SD38Zk8zmw00kbqOAKnhnO9I7+gXARek558P3GlmP02v44weNlsGPGmpB90b8MM+flsiGdHooyI9MLNmdy8NOodINunUkIhIjtMRgYhIjtMRgYhIjlMhEBHJcSoEIiI5ToVARCTHqRCIiOS4/w8ncEaig4o+2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1b741",
   "metadata": {},
   "source": [
    "エポックが進むにつれて、認識精度は向上している。<br>\n",
    "また訓練データとテストデータの精度には差がほとんどなく、過学習が起きていない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beda50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "820px",
    "left": "97px",
    "top": "164px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
