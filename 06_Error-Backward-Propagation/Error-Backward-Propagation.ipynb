{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9173b4f",
   "metadata": {},
   "source": [
    "# 逆伝播のイメージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d188c",
   "metadata": {},
   "source": [
    "以下は、計算グラフによる逆伝播のイメージ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bbde0",
   "metadata": {},
   "source": [
    "![画像を表示できません。](image.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4256b47",
   "metadata": {},
   "source": [
    "## 連鎖律"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ed01e",
   "metadata": {},
   "source": [
    "$$\\dfrac{\\partial L}{\\partial x}=\\dfrac{\\partial L}{\\partial t} \\dfrac{\\partial t}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d35a24",
   "metadata": {},
   "source": [
    "$x$ に関する$L$の微分は、「 $t$ に関する$L$の微分」と「 $x$ に関する $t$ の微分」の積で表すことができる。<br>\n",
    "逆方向に、局所的な微分の値を乗算していくことで、それぞれの変数の微分の値が得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acde43c",
   "metadata": {},
   "source": [
    "## 加算ノード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb36c11",
   "metadata": {},
   "source": [
    "$L=x+y+z$ について<br>\n",
    "$$\\dfrac{\\partial L}{\\partial x}=1,\\quad \\dfrac{\\partial L}{\\partial y}=1,\\quad \\dfrac{\\partial L}{\\partial z}=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13fafb",
   "metadata": {},
   "source": [
    "上流から伝わった値に、1を乗算して下流に流す。すなわち伝わった微分の値を次のノードへ流すだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70822d83",
   "metadata": {},
   "source": [
    "## 乗算ノード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3c2ec",
   "metadata": {},
   "source": [
    "$L=xyz$ について\n",
    "$$\\dfrac{\\partial L}{\\partial x}=yz,\\quad \\dfrac{\\partial L}{\\partial y}=zx,\\quad \\dfrac{\\partial L}{\\partial z}=xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085cac4",
   "metadata": {},
   "source": [
    "上流から伝わった値に、順伝播の際の入力信号のうち、自分の値を除いた値の積を乗算して下流に流す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecb3d9",
   "metadata": {},
   "source": [
    "# レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187b188",
   "metadata": {},
   "source": [
    "## 逆伝播のイメージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5d1c5",
   "metadata": {},
   "source": [
    "上のイメージをPythonで実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801a0a2",
   "metadata": {},
   "source": [
    "### 加算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b164dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560eb82d",
   "metadata": {},
   "source": [
    "### 乗算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd62cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afe30b",
   "metadata": {},
   "source": [
    "### 行列乗算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3562135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW # Numpy配列上のメモリ位置を固定した上で、要素を上書き\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8ad1c",
   "metadata": {},
   "source": [
    "### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff745b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc19821",
   "metadata": {},
   "source": [
    "## 活性化関数レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7d281",
   "metadata": {},
   "source": [
    "ニューラルネットワークを構成する「層（レイヤ）」を実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82052c",
   "metadata": {},
   "source": [
    "### ReLUレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baaca8b",
   "metadata": {},
   "source": [
    "ReLU（Rectified Linear Unit）\n",
    "$$y=\\begin{cases}\n",
    "    x & (x > 0) \\\\\n",
    "    0 & (x \\leqq 0)\n",
    "  \\end{cases}$$\n",
    "$x$に関する$y$の微分は以下のようになる。\n",
    "$$\\dfrac{\\partial y}{\\partial x}=\\begin{cases}\n",
    "    1 & (x > 0) \\\\\n",
    "    0 & (x \\leqq 0)\n",
    "  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbfe20",
   "metadata": {},
   "source": [
    "順伝播時に、$x$ が0以下であれば、逆伝播では、下流への信号はストップする。<br>\n",
    "つまりReLUレイヤは、回路における「**スイッチ**」のように機能する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363422e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de478f",
   "metadata": {},
   "source": [
    "逆伝播では、順伝播時に保持した`mask`を使って、逆伝播された`dout`に対し、`mask`の要素が`True`（すなわち負の値）の場所を`0`に設定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51e35e",
   "metadata": {},
   "source": [
    "### Sigmoidレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45d561",
   "metadata": {},
   "source": [
    "$$y=\\dfrac{1}{1+\\exp \\left( -x\\right) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710aeac",
   "metadata": {},
   "source": [
    "step5「 $x$ 」　→　　step4「 $-x$ 」　→　　step3「 $\\exp(-x)$ 」　→　　step2「 $1+\\exp(-x)$ 」　→　　step1「 $\\dfrac{1}{1+\\exp \\left( -x\\right) }(=y)$ 」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afc5df",
   "metadata": {},
   "source": [
    "> **導出**\n",
    "\n",
    ">**step1**\n",
    ">\n",
    ">最終的な値$L$を、シグモイド間数の出力 $y$ で微分する。\n",
    ">$$\\dfrac{\\partial L}{\\partial y}\\tag{1}$$\n",
    "\n",
    ">**step2**\n",
    ">\n",
    ">逆数を返す $b=\\dfrac{1}{a}$ この式の微分は、次の式で表される。\n",
    ">$\\dfrac{\\partial b}{\\partial a} = -a^{-2} =-b^{2}$\n",
    ">\n",
    ">順伝播の出力の2乗に、マイナスを付けた値を乗算。\n",
    ">$$-\\dfrac{\\partial L}{\\partial y}y^{2}\\tag{2}$$\n",
    "\n",
    ">**step3**\n",
    ">\n",
    ">1を加える加算ノードでは、値をそのまま流す。\n",
    ">$$-\\dfrac{\\partial L}{\\partial y}y^{2}\\tag{3}$$\n",
    "\n",
    ">**step4**\n",
    ">\n",
    ">$b=\\exp(a)$ この式の微分は、次の式で表される。$\\dfrac{\\partial b}{\\partial a}=\\exp \\left( a\\right)$\n",
    ">\n",
    ">ネイピア数の、順伝播時の入力の値乗（順伝播時の出力値）を乗算。\n",
    ">$$-\\dfrac{\\partial L}{\\partial y}y^{2}\\exp \\left( -x\\right) \\tag{4}$$\n",
    "\n",
    ">**step5**\n",
    ">\n",
    ">順伝播の際の入力信号のうち、自分の値を除いた値の積（-1）を乗算し、変形。<br><br>\n",
    ">$$\\begin{align}\n",
    "\\dfrac{\\partial L}{\\partial y}y^{2}\\exp \\left( -x\\right)\n",
    "&= \\dfrac{\\partial L}{\\partial y}\\dfrac{1}{\\lbrace(1+\\exp \\left( -x\\right)\\rbrace^{2}} \\exp \\left( -x\\right)\\\\\\\\\n",
    "&= \\dfrac{\\partial L}{\\partial y}\\dfrac{1}{1+\\exp \\left( -x\\right)} \\dfrac{\\exp \\left( -x\\right)}{1+\\exp \\left( -x\\right)}\\\\\\\\\n",
    "&= \\dfrac{\\partial L}{\\partial y}y\\left( 1-y\\right) \\tag{5} \n",
    "\\end{align}$$<br>\n",
    ">Sigmoidレイヤの逆伝播は、順伝播の出力だけ（`x`を使わずに`y`のみ）から、シンプルに計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d76eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e775a",
   "metadata": {},
   "source": [
    "順伝播時の出力を`out`に保持し、逆伝播時に`out`を使って計算する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba029f9d",
   "metadata": {},
   "source": [
    "## Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e4e67",
   "metadata": {},
   "source": [
    "$\\mathrm{X}\\cdot \\mathrm{W}=\\mathrm{Y}$（多次元配列）について\n",
    "$$\\dfrac{\\partial L}{\\partial \\mathrm{X}}=\\dfrac{\\partial L}{\\partial \\mathrm{Y}} \\cdot \\mathrm{W}^\\mathsf{T},\\quad \n",
    "\\dfrac{\\partial L}{\\partial \\mathrm{W}}=\\mathrm{X}^\\mathsf{T} \\cdot \\dfrac{\\partial L}{\\partial \\mathrm{Y}}$$\n",
    "$\\mathsf{T}$は転置を表し、次元数を揃えるために転置を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9f0c2a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47313e",
   "metadata": {},
   "source": [
    "## Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c170b",
   "metadata": {},
   "source": [
    "ここでは、交差エントロピー誤差も含めて実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc01e6",
   "metadata": {},
   "source": [
    "![画像を表示できません。](image2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4cc7b0",
   "metadata": {},
   "source": [
    "分類問題において<br>\n",
    "「**ソフトマックス関数**」の損失関数に「**交差エントロピー誤差**」を用いると、逆伝播が $(y_1-t_1, \\quad y_2-t_2, \\quad y_3-t_3)$ とキレイな結果になる。<br><br>\n",
    "回帰問題において<br>\n",
    "「**恒等関数**」の損失関数に「**二乗和誤差**」を用いると、逆伝播が $(y_1-t_1,\\quad  y_2-t_2, \\quad y_3-t_3)$ とキレイな結果になる。<br><br>\n",
    "これらは偶然ではなく、逆伝播がキレイな値になるよう誤差関数が定義されたものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23aa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bb2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e1f70",
   "metadata": {},
   "source": [
    "教師ラベル：$(0, 1, 0)$<br>\n",
    "1. Softmaxレイヤの出力：$(0.3, 0.2, 0.5)$<br>\n",
    "$(0.3, -0.8, 0.5)$という大きな誤差を伝播するため、これより前のレイヤが学習する内容は大きくなる。<br><br>\n",
    "2. Softmaxレイヤの出力：$(0.01, 0.99, 0)$<br>\n",
    "$(0.01, -0.01, 0)$という小さな誤差を伝播するため、これより前のレイヤが学習する内容は大きくなる。<br><br>\n",
    "誤差の大きさによって、学習する量が変わるところが特徴。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e24b1e",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651a1dd",
   "metadata": {},
   "source": [
    "レイヤを組み合わせることで、ニューラルネットワークを構築できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab72c9f",
   "metadata": {},
   "source": [
    "**ニューラルネットワークの全体像**\n",
    ">1. ミニバッチ<br>\n",
    ">訓練データの中からランダムに一部のデータを選び出す。<br><br>\n",
    ">2. 勾配の算出（**誤差逆伝播法が活躍**）<br>\n",
    ">各重みパラメータに関する損失関数の勾配を求める。<br><br>\n",
    ">3. パラメータの更新<br>\n",
    ">重みパラメータを勾配方向に微少量だけ更新する。<br><br>\n",
    ">4. 繰り返し<br>\n",
    ">1〜3を繰り返す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d624d7",
   "metadata": {},
   "source": [
    "**TwoLayerNetクラスのメソッド**\n",
    "\n",
    "| メソッド | 説明 | 引数 |\n",
    "|:------|:----|:----|\n",
    "| `init(self, input_size, hidden_size, output_size)` |初期化|入力ニューロン数, 中間ニューロン数, 出力ニューロン数|\n",
    "| `predict(self, x)` |認識（推論）|画像データ|\n",
    "| `loss(self, x, t`) |損失関数|画像データ, 正解ラベル|\n",
    "| `accuracy(self, x, t)` |認識精度|画像データ, 正解ラベル|\n",
    "| `numerical_gradient(self, x, t)` |重みパラメータに対する勾配を求める|画像データ, 正解ラベル|\n",
    "| `gradient(self, x, t)` |誤差逆伝播法|画像データ, 正解ラベル|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24320407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9230e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88648310",
   "metadata": {},
   "source": [
    "## 勾配確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9126e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:5.836673595936918e-10\n",
      "b1:2.7897869176655814e-09\n",
      "W2:7.1059361108538524e-09\n",
      "b2:1.4060401403725998e-07\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 各重みの絶対誤差の平均を求める\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd11ea",
   "metadata": {},
   "source": [
    "数値微分と誤差逆伝播法でそれぞれ求めた勾配の差はかなり小さいことから、誤差逆伝播法の実装に誤りがないことになる。<br>\n",
    "（コンピュータでは、有限の精度で計算（例えば32ビットの浮動小数点）が行われるため、誤差は0にならない。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c527da",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58422089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.08235, 0.0837\n",
      "train acc, test acc | 0.9037166666666666, 0.9033\n",
      "train acc, test acc | 0.9223, 0.9224\n",
      "train acc, test acc | 0.9371333333333334, 0.9359\n",
      "train acc, test acc | 0.9449666666666666, 0.941\n",
      "train acc, test acc | 0.95195, 0.9491\n",
      "train acc, test acc | 0.9569833333333333, 0.9524\n",
      "train acc, test acc | 0.96135, 0.9565\n",
      "train acc, test acc | 0.9642833333333334, 0.9571\n",
      "train acc, test acc | 0.9664166666666667, 0.9608\n",
      "train acc, test acc | 0.9691166666666666, 0.9638\n",
      "train acc, test acc | 0.9722, 0.9655\n",
      "train acc, test acc | 0.97325, 0.9672\n",
      "train acc, test acc | 0.9749833333333333, 0.9661\n",
      "train acc, test acc | 0.9762833333333333, 0.9678\n",
      "train acc, test acc | 0.97735, 0.9685\n",
      "train acc, test acc | 0.97905, 0.9699\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e0d9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1b741",
   "metadata": {},
   "source": [
    "エポックが進むにつれて、認識精度は向上している。<br>\n",
    "また訓練データとテストデータの精度には差がほとんどなく、過学習が起きていない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beda50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "820px",
    "left": "97px",
    "top": "164px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
